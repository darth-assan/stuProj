{"cells":[{"cell_type":"code","execution_count":2,"id":"7b35306b-c386-4a3d-81b6-0fddebfed3a3","metadata":{"editable":false,"execution":{"iopub.execute_input":"2024-11-05T19:42:03.233965Z","iopub.status.busy":"2024-11-05T19:42:03.233580Z","iopub.status.idle":"2024-11-05T19:42:03.239600Z","shell.execute_reply":"2024-11-05T19:42:03.238620Z","shell.execute_reply.started":"2024-11-05T19:42:03.233927Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","import torch\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","\n","\n","# # Load your data\n","\n","# data_path = '/kaggle/input/gan3-training-data/train4_clean.csv'  # Update with the actual path\n","\n","# data = pd.read_csv(data_path)\n","\n","# data = data.drop(['Attack'], axis = 1)\n","\n","\n","# # Remove the timestamp if it's in your data\n","\n","# if 'timestamp' in data.columns:\n","\n","#     data = data.drop(columns=['timestamp'])\n","\n","\n","\n","# # Normalize the data\n","\n","# scaler = MinMaxScaler(feature_range=(-1, 1))\n","\n","# normalized_data = scaler.fit_transform(data.values)\n","\n","\n","\n","# # Convert to PyTorch tensor\n","\n","# tensor_data = torch.tensor(normalized_data, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n","\n","# train_loader = DataLoader(TensorDataset(tensor_data), batch_size=64, shuffle=True)\n"]},{"cell_type":"code","execution_count":3,"id":"5b017a8e-5548-47ae-be95-02bb33f75fec","metadata":{"editable":false,"execution":{"iopub.execute_input":"2024-11-05T19:42:04.670523Z","iopub.status.busy":"2024-11-05T19:42:04.670119Z","iopub.status.idle":"2024-11-05T19:42:07.473289Z","shell.execute_reply":"2024-11-05T19:42:07.472307Z","shell.execute_reply.started":"2024-11-05T19:42:04.670483Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(86401, 43)\n","Common columns between datasets: Index(['P1_B2016', 'P1_FCV03D', 'P1_FCV03Z', 'P1_FT01', 'P1_FT01Z', 'P1_FT02',\n","       'P1_FT02Z', 'P1_FT03', 'P1_FT03Z', 'P1_LIT01', 'P1_PCV02D', 'P1_PCV02Z',\n","       'P1_PIT01', 'P1_PIT02', 'P1_TIT01', 'P1_TIT02', 'P2_24Vdc', 'P2_AutoGO',\n","       'P2_SIT01', 'P2_TripEx', 'P3_FIT01', 'P3_LCP01D', 'P3_LCV01D',\n","       'P3_LIT01', 'P3_PIT01', 'P4_HT_FD', 'P4_HT_PO', 'P4_ST_FD', 'P4_ST_GOV',\n","       'P4_ST_LD', 'P4_ST_PO', 'P4_ST_PT01', 'P4_ST_TT01'],\n","      dtype='object')\n","There are 33\n"]},{"data":{"text/plain":["(86401, 33)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["#------------------------------------------\n","# Processing Data for GAN training\n","#------------------------------------------\n","import pandas as pd\n","import torch\n","\n","# Load the selected datasets\n","train_set_1 = pd.read_csv('/kaggle/input/gan3-training-data/train1_clean.csv')\n","train_set_2 = pd.read_csv('/kaggle/input/gan3-training-data/train4_clean.csv')\n","\n","print(train_set_2.shape)\n","\n","# Step 1: Find common columns\n","common_columns = train_set_1.columns.intersection(train_set_2.columns)\n","print(f\"Common columns between datasets: {common_columns}\")\n","print(f\"There are {len(common_columns)}\")\n","\n","# Filter both datasets to keep only the common columns\n","train_set_1 = train_set_1[common_columns]\n","train_set_2 = train_set_2[common_columns]\n","\n","# print(f\"{train_set_1.head()}\")\n","# print(f\"{train_set_2.head()}\")\n","train_set_2.shape"]},{"cell_type":"code","execution_count":4,"id":"56c225be-5a5b-4805-949f-f278d2edc14f","metadata":{"editable":false,"execution":{"iopub.execute_input":"2024-11-05T19:42:07.476007Z","iopub.status.busy":"2024-11-05T19:42:07.475285Z","iopub.status.idle":"2024-11-05T19:42:07.515911Z","shell.execute_reply":"2024-11-05T19:42:07.515078Z","shell.execute_reply.started":"2024-11-05T19:42:07.475959Z"},"trusted":true},"outputs":[],"source":["# Normalize the data\n","\n","scaler = MinMaxScaler(feature_range=(-1, 1))\n","\n","normalized_data = scaler.fit_transform(train_set_2.values)\n","\n","\n","\n","# Convert to PyTorch tensor\n","\n","tensor_data = torch.tensor(normalized_data, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n","\n","train_loader = DataLoader(TensorDataset(tensor_data), batch_size=64, shuffle=True)"]},{"cell_type":"code","execution_count":5,"id":"9064aed0-1b04-4283-a550-c7b84c3ca5e6","metadata":{"editable":false,"execution":{"iopub.execute_input":"2024-11-05T19:42:07.517563Z","iopub.status.busy":"2024-11-05T19:42:07.517154Z","iopub.status.idle":"2024-11-05T19:53:03.713644Z","shell.execute_reply":"2024-11-05T19:53:03.712672Z","shell.execute_reply.started":"2024-11-05T19:42:07.517514Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/50], Batch [1], d_loss: 1.3991, g_loss: 0.7896\n","Epoch [1/50], Batch [1001], d_loss: 1.2667, g_loss: 0.9779\n","Epoch [2/50], Batch [1], d_loss: 1.3028, g_loss: 0.7847\n","Epoch [2/50], Batch [1001], d_loss: 1.3365, g_loss: 0.7805\n","Epoch [3/50], Batch [1], d_loss: 1.2634, g_loss: 0.7876\n","Epoch [3/50], Batch [1001], d_loss: 1.0178, g_loss: 1.3465\n","Epoch [4/50], Batch [1], d_loss: 0.9522, g_loss: 1.2348\n","Epoch [4/50], Batch [1001], d_loss: 0.9421, g_loss: 1.4525\n","Epoch [5/50], Batch [1], d_loss: 0.8441, g_loss: 1.9325\n","Epoch [5/50], Batch [1001], d_loss: 0.8354, g_loss: 1.6994\n","Epoch [6/50], Batch [1], d_loss: 0.8919, g_loss: 1.7740\n","Epoch [6/50], Batch [1001], d_loss: 0.7530, g_loss: 1.7965\n","Epoch [7/50], Batch [1], d_loss: 0.7779, g_loss: 1.9778\n","Epoch [7/50], Batch [1001], d_loss: 0.6791, g_loss: 1.8916\n","Epoch [8/50], Batch [1], d_loss: 0.7421, g_loss: 2.2674\n","Epoch [8/50], Batch [1001], d_loss: 0.7313, g_loss: 1.9498\n","Epoch [9/50], Batch [1], d_loss: 0.6855, g_loss: 2.0632\n","Epoch [9/50], Batch [1001], d_loss: 0.6717, g_loss: 2.1037\n","Epoch [10/50], Batch [1], d_loss: 0.7208, g_loss: 1.9510\n","Epoch [10/50], Batch [1001], d_loss: 0.6854, g_loss: 1.8653\n","Epoch [11/50], Batch [1], d_loss: 0.7298, g_loss: 2.0931\n","Epoch [11/50], Batch [1001], d_loss: 0.6781, g_loss: 1.9627\n","Epoch [12/50], Batch [1], d_loss: 0.6766, g_loss: 1.9429\n","Epoch [12/50], Batch [1001], d_loss: 0.6624, g_loss: 2.1375\n","Epoch [13/50], Batch [1], d_loss: 0.6711, g_loss: 2.5326\n","Epoch [13/50], Batch [1001], d_loss: 0.6526, g_loss: 2.1017\n","Epoch [14/50], Batch [1], d_loss: 0.6617, g_loss: 1.9512\n","Epoch [14/50], Batch [1001], d_loss: 0.6507, g_loss: 2.0607\n","Epoch [15/50], Batch [1], d_loss: 0.6512, g_loss: 2.0702\n","Epoch [15/50], Batch [1001], d_loss: 0.6504, g_loss: 2.0822\n","Epoch [16/50], Batch [1], d_loss: 0.6519, g_loss: 2.0350\n","Epoch [16/50], Batch [1001], d_loss: 0.6503, g_loss: 2.0840\n","Epoch [17/50], Batch [1], d_loss: 0.6506, g_loss: 2.1334\n","Epoch [17/50], Batch [1001], d_loss: 0.6504, g_loss: 2.0839\n","Epoch [18/50], Batch [1], d_loss: 0.6514, g_loss: 2.1137\n","Epoch [18/50], Batch [1001], d_loss: 0.6503, g_loss: 2.0785\n","Epoch [19/50], Batch [1], d_loss: 0.6864, g_loss: 2.0811\n","Epoch [19/50], Batch [1001], d_loss: 0.6730, g_loss: 2.0272\n","Epoch [20/50], Batch [1], d_loss: 0.7125, g_loss: 1.9782\n","Epoch [20/50], Batch [1001], d_loss: 0.6804, g_loss: 2.0051\n","Epoch [21/50], Batch [1], d_loss: 0.6539, g_loss: 2.1151\n","Epoch [21/50], Batch [1001], d_loss: 0.7106, g_loss: 1.9569\n","Epoch [22/50], Batch [1], d_loss: 0.6550, g_loss: 2.0273\n","Epoch [22/50], Batch [1001], d_loss: 0.6534, g_loss: 2.0347\n","Epoch [23/50], Batch [1], d_loss: 0.6528, g_loss: 2.0305\n","Epoch [23/50], Batch [1001], d_loss: 0.6746, g_loss: 2.0439\n","Epoch [24/50], Batch [1], d_loss: 0.8669, g_loss: 1.3995\n","Epoch [24/50], Batch [1001], d_loss: 0.6741, g_loss: 2.0128\n","Epoch [25/50], Batch [1], d_loss: 0.6651, g_loss: 2.1442\n","Epoch [25/50], Batch [1001], d_loss: 0.6571, g_loss: 2.0733\n","Epoch [26/50], Batch [1], d_loss: 0.6515, g_loss: 2.1225\n","Epoch [26/50], Batch [1001], d_loss: 0.6734, g_loss: 2.1635\n","Epoch [27/50], Batch [1], d_loss: 0.6814, g_loss: 2.1424\n","Epoch [27/50], Batch [1001], d_loss: 0.6517, g_loss: 2.0334\n","Epoch [28/50], Batch [1], d_loss: 0.6562, g_loss: 2.0927\n","Epoch [28/50], Batch [1001], d_loss: 0.6794, g_loss: 2.0480\n","Epoch [29/50], Batch [1], d_loss: 0.6835, g_loss: 2.0069\n","Epoch [29/50], Batch [1001], d_loss: 0.6605, g_loss: 2.0433\n","Epoch [30/50], Batch [1], d_loss: 0.6522, g_loss: 2.0655\n","Epoch [30/50], Batch [1001], d_loss: 0.6885, g_loss: 2.2115\n","Epoch [31/50], Batch [1], d_loss: 0.7384, g_loss: 2.0663\n","Epoch [31/50], Batch [1001], d_loss: 0.6511, g_loss: 2.0713\n","Epoch [32/50], Batch [1], d_loss: 0.6883, g_loss: 2.4302\n","Epoch [32/50], Batch [1001], d_loss: 0.6778, g_loss: 1.9579\n","Epoch [33/50], Batch [1], d_loss: 0.6823, g_loss: 2.1753\n","Epoch [33/50], Batch [1001], d_loss: 0.7305, g_loss: 2.2200\n","Epoch [34/50], Batch [1], d_loss: 0.6910, g_loss: 2.1173\n","Epoch [34/50], Batch [1001], d_loss: 0.6538, g_loss: 2.0149\n","Epoch [35/50], Batch [1], d_loss: 0.7145, g_loss: 2.1285\n","Epoch [35/50], Batch [1001], d_loss: 0.7025, g_loss: 2.0151\n","Epoch [36/50], Batch [1], d_loss: 0.6670, g_loss: 2.0707\n","Epoch [36/50], Batch [1001], d_loss: 0.6550, g_loss: 2.0726\n","Epoch [37/50], Batch [1], d_loss: 0.8005, g_loss: 1.7363\n","Epoch [37/50], Batch [1001], d_loss: 0.6702, g_loss: 1.9642\n","Epoch [38/50], Batch [1], d_loss: 0.6515, g_loss: 2.0017\n","Epoch [38/50], Batch [1001], d_loss: 0.6781, g_loss: 2.0060\n","Epoch [39/50], Batch [1], d_loss: 0.6809, g_loss: 2.0087\n","Epoch [39/50], Batch [1001], d_loss: 0.6528, g_loss: 2.0267\n","Epoch [40/50], Batch [1], d_loss: 0.6534, g_loss: 2.0574\n","Epoch [40/50], Batch [1001], d_loss: 0.6652, g_loss: 2.1240\n","Epoch [41/50], Batch [1], d_loss: 0.6582, g_loss: 2.0172\n","Epoch [41/50], Batch [1001], d_loss: 0.6656, g_loss: 2.0245\n","Epoch [42/50], Batch [1], d_loss: 0.6829, g_loss: 2.0023\n","Epoch [42/50], Batch [1001], d_loss: 0.6507, g_loss: 2.0560\n","Epoch [43/50], Batch [1], d_loss: 0.6798, g_loss: 2.0695\n","Epoch [43/50], Batch [1001], d_loss: 0.6506, g_loss: 2.1035\n","Epoch [44/50], Batch [1], d_loss: 0.6710, g_loss: 1.9930\n","Epoch [44/50], Batch [1001], d_loss: 0.6534, g_loss: 2.0727\n","Epoch [45/50], Batch [1], d_loss: 0.6598, g_loss: 2.0789\n","Epoch [45/50], Batch [1001], d_loss: 0.6505, g_loss: 2.0715\n","Epoch [46/50], Batch [1], d_loss: 0.6525, g_loss: 2.0815\n","Epoch [46/50], Batch [1001], d_loss: 0.6508, g_loss: 2.0522\n","Epoch [47/50], Batch [1], d_loss: 0.6792, g_loss: 2.0528\n","Epoch [47/50], Batch [1001], d_loss: 0.6510, g_loss: 2.0119\n","Epoch [48/50], Batch [1], d_loss: 0.9197, g_loss: 0.9260\n","Epoch [48/50], Batch [1001], d_loss: 0.7089, g_loss: 2.0145\n","Epoch [49/50], Batch [1], d_loss: 0.6508, g_loss: 2.0789\n","Epoch [49/50], Batch [1001], d_loss: 0.6612, g_loss: 2.0327\n","Epoch [50/50], Batch [1], d_loss: 0.8087, g_loss: 1.8878\n","Epoch [50/50], Batch [1001], d_loss: 0.6590, g_loss: 2.0261\n","Training complete. Use generator to create synthetic data.\n"]}],"source":["import torch\n","\n","import torch.nn as nn\n","\n","import torch.optim as optim\n","\n","import pandas as pd\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","\n","\n","# Set device to GPU if available\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","\n","# Parameters\n","\n","input_dim = 100               # Dimension of the input noise vector for the generator\n","\n","feature_dim = 16              # Adjusted base feature size for convolutional layers\n","\n","num_epochs = 50            # Number of training epochs for longer training\n","\n","batch_size = 32               # Batch size\n","\n","generator_lr = 0.0003         # Increased learning rate for the generator\n","\n","discriminator_lr = 0.0001     # Decreased learning rate for the discriminator\n","\n","noise_std_dev = 0.01          # Standard deviation for noise added to real data\n","\n","\n","\n","# Discriminator Model\n","\n","class Discriminator(nn.Module):\n","\n","    def __init__(self):\n","\n","        super(Discriminator, self).__init__()\n","\n","        self.conv_layers = nn.Sequential(\n","\n","            nn.Conv1d(1, feature_dim, kernel_size=4, stride=2, padding=1),\n","\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Conv1d(feature_dim, feature_dim * 2, kernel_size=4, stride=2, padding=1),\n","\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Dropout(0.5),\n","\n","            nn.Conv1d(feature_dim * 2, feature_dim * 4, kernel_size=3, stride=2, padding=1),\n","\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Dropout(0.5),\n","\n","            nn.Conv1d(feature_dim * 4, feature_dim * 8, kernel_size=3, stride=2, padding=1),\n","\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Conv1d(feature_dim * 8, feature_dim * 16, kernel_size=2, stride=2, padding=1),\n","\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Conv1d(feature_dim * 16, feature_dim * 32, kernel_size=2, stride=2, padding=1),\n","\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Conv1d(feature_dim * 32, 1, kernel_size=2, stride=1, padding=0)\n","\n","        )\n","\n","        self.fc_layers = nn.Sequential(\n","\n","            nn.Linear(1, 1),\n","\n","            nn.Sigmoid()\n","\n","        )\n","\n","\n","\n","    def forward(self, x):\n","\n","        x = self.conv_layers(x)\n","\n","        x = x.view(x.size(0), -1)\n","\n","        return self.fc_layers(x)\n","\n","\n","\n","# Generator Model\n","\n","class Generator(nn.Module):\n","\n","    def __init__(self):\n","\n","        super(Generator, self).__init__()\n","\n","        self.fc_layers = nn.Sequential(\n","\n","            nn.Linear(input_dim, feature_dim * 16),\n","\n","            nn.ReLU(),\n","\n","            nn.Linear(feature_dim * 16, feature_dim * 8),\n","\n","            nn.ReLU(),\n","\n","            nn.Linear(feature_dim * 8, feature_dim * 4),\n","\n","            nn.ReLU()\n","\n","        )\n","\n","        self.deconv_layers = nn.Sequential(\n","\n","            nn.Upsample(scale_factor=2),\n","\n","            nn.ConvTranspose1d(feature_dim * 4, feature_dim * 2, 4, stride=2, padding=1),\n","\n","            nn.ReLU(),\n","\n","            nn.Upsample(scale_factor=2),\n","\n","            nn.ConvTranspose1d(feature_dim * 2, feature_dim, 4, stride=2, padding=1),\n","\n","            nn.ReLU(),\n","\n","            nn.ConvTranspose1d(feature_dim, 1, kernel_size=4, stride=2, padding=1),\n","\n","            nn.Tanh()  # Output in range [-1, 1]\n","\n","        )\n","\n","\n","\n","    def forward(self, x):\n","\n","        x = self.fc_layers(x)\n","\n","        x = x.view(x.size(0), -1, 1)\n","\n","        return self.deconv_layers(x)\n","\n","\n","\n","# Initialize models\n","\n","discriminator = Discriminator().to(device)\n","\n","generator = Generator().to(device)\n","\n","\n","\n","# Loss and Optimizer\n","\n","criterion = nn.BCELoss()  # Binary Cross Entropy for GAN\n","\n","optimizer_d = optim.Adam(discriminator.parameters(), lr=discriminator_lr, betas=(0.5, 0.999))\n","\n","optimizer_g = optim.Adam(generator.parameters(), lr=generator_lr, betas=(0.5, 0.999))\n","\n","\n","\n","# Training the GAN with added noise to real data for regularization\n","\n","for epoch in range(num_epochs):\n","\n","    for i, batch in enumerate(train_loader):\n","\n","        real_data = batch[0].to(device)\n","\n","\n","\n","        # Train Discriminator\n","\n","        optimizer_d.zero_grad()\n","\n","\n","\n","        # Add slight noise to real data to regularize the discriminator\n","\n","        real_data_noisy = real_data + torch.normal(0, noise_std_dev, real_data.shape).to(device)\n","\n","\n","\n","        # Real and fake labels\n","\n","        real_labels = torch.full((real_data.size(0), 1), 0.9).to(device)  # Label smoothing for real data\n","\n","        fake_labels = torch.full((real_data.size(0), 1), 0.1).to(device)  # Label smoothing for fake data\n","\n","\n","\n","        # Discriminator on real data with noise\n","\n","        outputs = discriminator(real_data_noisy)\n","\n","        d_loss_real = criterion(outputs, real_labels)\n","\n","\n","\n","        # Discriminator on fake data\n","\n","        noise = torch.randn(real_data.size(0), input_dim).to(device)\n","\n","        fake_data = generator(noise)\n","\n","        outputs = discriminator(fake_data.detach())\n","\n","        d_loss_fake = criterion(outputs, fake_labels)\n","\n","\n","\n","        # Backprop and optimize discriminator\n","\n","        d_loss = d_loss_real + d_loss_fake\n","\n","        d_loss.backward()\n","\n","        optimizer_d.step()\n","\n","\n","\n","        # Train Generator\n","\n","        optimizer_g.zero_grad()\n","\n","        outputs = discriminator(fake_data)\n","\n","        g_loss = criterion(outputs, real_labels)  # Fool discriminator into thinking fake data is real\n","\n","\n","\n","        # Backprop and optimize generator\n","\n","        g_loss.backward()\n","\n","        optimizer_g.step()\n","\n","\n","\n","        # Print progress every 10 batches\n","        if i % 1000 == 0:\n","            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}\")\n","\n","        # if (epoch + 1) % 10 == 0:\n","    \n","        #     print(f\"Epoch [{epoch+1}/{num_epochs}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}\")\n","\n","\n","\n","print(\"Training complete. Use generator to create synthetic data.\")\n"]},{"cell_type":"code","execution_count":5,"id":"732e114b-ca3d-4904-b0e0-8ebcff9c1735","metadata":{"editable":false},"outputs":[{"name":"stdout","output_type":"stream","text":["Synthetic data saved as 'synthetic_data.csv'\n"]}],"source":["# Generate synthetic data for three days\n","\n","samples_per_day = 86201  # Update based on your data\n","\n","days_to_generate = 3\n","\n","total_samples = samples_per_day * days_to_generate\n","\n","\n","\n","noise = torch.randn(total_samples, input_dim).to(device)\n","\n","synthetic_data = generator(noise).detach().cpu().numpy()\n","\n","synthetic_data_reshaped = synthetic_data.reshape(total_samples, -1)\n","\n","\n","\n","# Save to CSV\n","\n","synthetic_df = pd.DataFrame(synthetic_data_reshaped)\n","\n","synthetic_df.to_csv(\"synthetic_data.csv\", index=False)\n","\n","\n","\n","print(\"Synthetic data saved as 'synthetic_data.csv'\")\n"]},{"cell_type":"code","execution_count":7,"id":"9181e372-0823-4296-b85a-0307d429873f","metadata":{"editable":false},"outputs":[{"name":"stdout","output_type":"stream","text":["Combined dataset shape: (86701, 32)\n"]}],"source":["import pandas as pd\n","\n","\n","\n","# Load the second train set (train_set_2) as a DataFrame\n","\n","train_set_2_path = 'G:/stuProj/data/train4_clean.csv'  # Update with the actual path\n","\n","train_set_2 = pd.read_csv(train_set_2_path)\n","\n","\n","\n","# Ensure only relevant columns (assumes synthetic data has 32 columns)\n","\n","train_set_2 = train_set_2[train_set_2.columns[:32]]  # Adjust column count as needed\n","\n","\n","\n","# Reshape synthetic data and convert to DataFrame\n","\n","synthetic_data_reshaped = synthetic_data.reshape(synthetic_data.shape[0], -1)\n","\n","synthetic_data_df = pd.DataFrame(synthetic_data_reshaped, columns=train_set_2.columns)\n","\n","\n","\n","# Concatenate synthetic data to train_set_2\n","\n","combined_data = pd.concat([train_set_2, synthetic_data_df], ignore_index=True)\n","\n","print(\"Combined dataset shape:\", combined_data.shape)\n"]},{"cell_type":"code","execution_count":9,"id":"f57096b5-c05d-43ce-9711-f97b85edb74f","metadata":{"editable":false},"outputs":[],"source":["# Create two three-day subsets from the original train_set_2\n","\n","samples_per_day = 100  # Adjust based on your data\n","\n","subset_1 = train_set_2.iloc[:samples_per_day * 3]  # First three days\n","\n","subset_2 = train_set_2.iloc[samples_per_day * 3:samples_per_day * 6]  # Second three days\n","\n","\n","\n","# The third set is the synthetic three-day data\n","\n","subset_3 = synthetic_data_df\n"]},{"cell_type":"code","execution_count":11,"id":"1b42e440-4684-40a5-b7fd-55a23ad7573f","metadata":{"editable":false},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","\n","\n","# Define a MinMaxScaler instance\n","\n","scaler = MinMaxScaler()\n","\n","\n","\n","# Normalize each subset\n","\n","normalized_subset_1 = pd.DataFrame(scaler.fit_transform(subset_1), columns=subset_1.columns)\n","\n","normalized_subset_2 = pd.DataFrame(scaler.fit_transform(subset_2), columns=subset_2.columns)\n","\n","normalized_subset_3 = pd.DataFrame(scaler.fit_transform(subset_3), columns=subset_3.columns)\n"]},{"cell_type":"code","execution_count":13,"id":"6f4c49e2-dfa5-4247-af55-5bb2be60665a","metadata":{"editable":false},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of sensors passing K-S test (real-real): 12\n","Number of sensors passing K-S test (real-synthetic 1): 5\n","Number of sensors passing K-S test (real-synthetic 2): 4\n"]}],"source":["from scipy.stats import ks_2samp\n","\n","\n","\n","# Function to calculate K-S statistics between each pair of datasets\n","\n","def compute_ks_statistics(df1, df2):\n","\n","    ks_results = []\n","\n","    for column in df1.columns:\n","\n","        ks_stat, p_value = ks_2samp(df1[column], df2[column])\n","\n","        ks_results.append({\n","\n","            'sensor': column,\n","\n","            'ks_statistic': ks_stat,\n","\n","            'p_value': p_value,\n","\n","            'passes': ks_stat < 0.15 and p_value > 0.03  # Define pass criteria\n","\n","        })\n","\n","    return pd.DataFrame(ks_results)\n","\n","\n","\n","# Compare normalized subsets\n","\n","ks_results_real_real = compute_ks_statistics(normalized_subset_1, normalized_subset_2)\n","\n","ks_results_real_synthetic = compute_ks_statistics(normalized_subset_1, normalized_subset_3)\n","\n","ks_results_real_synthetic_2 = compute_ks_statistics(normalized_subset_2, normalized_subset_3)\n","\n","\n","\n","# Count the number of sensors that pass for each comparison\n","\n","passes_real_real = ks_results_real_real['passes'].sum()\n","\n","passes_real_synthetic = ks_results_real_synthetic['passes'].sum()\n","\n","passes_real_synthetic_2 = ks_results_real_synthetic_2['passes'].sum()\n","\n","\n","\n","print(\"Number of sensors passing K-S test (real-real):\", passes_real_real)\n","\n","print(\"Number of sensors passing K-S test (real-synthetic 1):\", passes_real_synthetic)\n","\n","print(\"Number of sensors passing K-S test (real-synthetic 2):\", passes_real_synthetic_2)\n"]},{"cell_type":"code","execution_count":15,"id":"e66363d5-9eb6-42d8-82bb-f05f4cd2b1e5","metadata":{"editable":false},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Detailed K-S Test Results (Real-Real):\n","        sensor  ks_statistic        p_value  passes\n","0     P1_B2016      0.106667   6.580850e-02    True\n","1    P1_FCV03D      0.136667   7.311227e-03   False\n","2    P1_FCV03Z      0.550000   4.911614e-42   False\n","3      P1_FT01      0.290000   1.598960e-11   False\n","4     P1_FT01Z      0.290000   1.598960e-11   False\n","5      P1_FT02      0.810000   5.295808e-99   False\n","6     P1_FT02Z      0.810000   5.295808e-99   False\n","7      P1_FT03      0.646667   2.030859e-59   False\n","8     P1_FT03Z      0.650000   4.348910e-60   False\n","9     P1_LIT01      0.213333   2.166454e-06   False\n","10   P1_PCV02D      0.000000   1.000000e+00    True\n","11   P1_PCV02Z      0.916667  1.634256e-135   False\n","12    P1_PIT01      0.243333   3.324740e-08   False\n","13    P1_PIT02      0.606667   9.173810e-52   False\n","14     P1_PP04      0.996667  8.881787e-177   False\n","15   P1_PP04SP      0.000000   1.000000e+00    True\n","16    P1_TIT01      0.383333   4.914991e-20   False\n","17    P1_TIT02      0.073333   3.958303e-01    True\n","18    P1_TIT03      0.000000   1.000000e+00    True\n","19    P2_24Vdc      0.126667   1.616248e-02   False\n","20   P2_AutoGO      0.000000   1.000000e+00    True\n","21      P2_SCO      0.033333   9.963667e-01    True\n","22     P2_SCST      0.340000   9.158955e-16   False\n","23    P2_SIT01      0.676667   1.261757e-65   False\n","24   P2_TripEx      0.000000   1.000000e+00    True\n","25  P2_VIBTR01      0.026667   9.999325e-01    True\n","26  P2_VIBTR02      0.050000   8.483422e-01    True\n","27  P2_VIBTR03      0.040000   9.704672e-01    True\n","28  P2_VIBTR04      0.073333   3.958303e-01    True\n","29    P3_FIT01      0.486667   1.415349e-32   False\n","30   P3_LCP01D      0.406667   1.433426e-22   False\n","31   P3_LCV01D      0.346667   2.194274e-16   False\n","\n","Detailed K-S Test Results (Real-Synthetic 1):\n","        sensor  ks_statistic        p_value  passes\n","0     P1_B2016      0.826667  5.483672e-104   False\n","1    P1_FCV03D      0.996667  8.881787e-177   False\n","2    P1_FCV03Z      0.996667  8.881787e-177   False\n","3      P1_FT01      0.926667  1.174013e-139   False\n","4     P1_FT01Z      0.996667  8.881787e-177   False\n","5      P1_FT02      0.843333  3.367452e-109   False\n","6     P1_FT02Z      0.966667  2.287458e-158   False\n","7      P1_FT03      0.880000  1.405984e-121   False\n","8     P1_FT03Z      0.993333  2.660095e-174   False\n","9     P1_LIT01      0.790000   2.698982e-93   False\n","10   P1_PCV02D      0.000000   1.000000e+00    True\n","11   P1_PCV02Z      0.996667  8.881787e-177   False\n","12    P1_PIT01      0.996667  8.881787e-177   False\n","13    P1_PIT02      0.990000  5.302457e-172   False\n","14     P1_PP04      0.000000   1.000000e+00    True\n","15   P1_PP04SP      0.000000   1.000000e+00    True\n","16    P1_TIT01      0.993333  2.660095e-174   False\n","17    P1_TIT02      0.996667  8.881787e-177   False\n","18    P1_TIT03      0.486667   1.415349e-32   False\n","19    P2_24Vdc      0.996667  8.881787e-177   False\n","20   P2_AutoGO      0.000000   1.000000e+00    True\n","21      P2_SCO      0.993333  2.660095e-174   False\n","22     P2_SCST      0.993333  2.660095e-174   False\n","23    P2_SIT01      0.960000  6.022081e-155   False\n","24   P2_TripEx      0.000000   1.000000e+00    True\n","25  P2_VIBTR01      0.860000  1.166605e-114   False\n","26  P2_VIBTR02      0.920000  7.093124e-137   False\n","27  P2_VIBTR03      0.996667  8.881787e-177   False\n","28  P2_VIBTR04      0.950000  4.461665e-150   False\n","29    P3_FIT01      0.836667  4.370472e-107   False\n","30   P3_LCP01D      0.970000  3.870488e-160   False\n","31   P3_LCV01D      0.686667   8.667022e-68   False\n","\n","Detailed K-S Test Results (Real-Synthetic 2):\n","        sensor  ks_statistic        p_value  passes\n","0     P1_B2016      0.736667   2.282398e-79   False\n","1    P1_FCV03D      0.996667  8.881787e-177   False\n","2    P1_FCV03Z      0.996667  8.881787e-177   False\n","3      P1_FT01      0.896667  1.085389e-127   False\n","4     P1_FT01Z      0.996667  8.881787e-177   False\n","5      P1_FT02      0.936667  5.559839e-144   False\n","6     P1_FT02Z      0.903333  3.101396e-130   False\n","7      P1_FT03      0.900000  5.902991e-129   False\n","8     P1_FT03Z      0.996667  8.881787e-177   False\n","9     P1_LIT01      0.793333   3.160518e-94   False\n","10   P1_PCV02D      0.000000   1.000000e+00    True\n","11   P1_PCV02Z      0.996667  8.881787e-177   False\n","12    P1_PIT01      0.986667  7.913916e-170   False\n","13    P1_PIT02      0.996667  8.881787e-177   False\n","14     P1_PP04      0.996667  8.881787e-177   False\n","15   P1_PP04SP      0.000000   1.000000e+00    True\n","16    P1_TIT01      0.996667  8.881787e-177   False\n","17    P1_TIT02      0.996667  8.881787e-177   False\n","18    P1_TIT03      0.486667   1.415349e-32   False\n","19    P2_24Vdc      0.996667  8.881787e-177   False\n","20   P2_AutoGO      0.000000   1.000000e+00    True\n","21      P2_SCO      0.993333  2.660095e-174   False\n","22     P2_SCST      0.996667  8.881787e-177   False\n","23    P2_SIT01      0.960000  6.022081e-155   False\n","24   P2_TripEx      0.000000   1.000000e+00    True\n","25  P2_VIBTR01      0.866667  6.417372e-117   False\n","26  P2_VIBTR02      0.920000  7.093124e-137   False\n","27  P2_VIBTR03      0.996667  8.881787e-177   False\n","28  P2_VIBTR04      0.956667  2.723834e-153   False\n","29    P3_FIT01      0.853333  1.916426e-112   False\n","30   P3_LCP01D      0.563333   3.265629e-44   False\n","31   P3_LCV01D      0.520000   2.238638e-37   False\n"]}],"source":["print(\"\\nDetailed K-S Test Results (Real-Real):\")\n","\n","print(ks_results_real_real)\n","\n","\n","\n","print(\"\\nDetailed K-S Test Results (Real-Synthetic 1):\")\n","\n","print(ks_results_real_synthetic)\n","\n","\n","\n","print(\"\\nDetailed K-S Test Results (Real-Synthetic 2):\")\n","\n","print(ks_results_real_synthetic_2)\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":6018138,"sourceId":9815909,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":5}
